name: Crawl and Post (All-in-One)

on:
  schedule:
    # 매일 오전 10시, 오후 3시, 오후 8시 실행 (KST)
    - cron: '0 1,6,11 * * *'  # UTC 기준
  workflow_dispatch:  # 수동 실행 가능
    inputs:
      skip_crawl:
        description: 'Skip crawling (only post existing)'
        required: false
        type: boolean
        default: false

jobs:
  crawl-and-post:
    runs-on: ubuntu-latest
    timeout-minutes: 45
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20'
        cache: 'npm'
    
    - name: Install dependencies
      run: |
        npm config set loglevel warn
        npm config set progress false
        npm ci --prefer-offline --no-optional || npm install --legacy-peer-deps --no-audit --no-fund
      timeout-minutes: 8
    
    - name: Install browsers
      run: |
        # Playwright Chrome 설치
        npx playwright install chromium
        
        # Chrome 브라우저 설치 (Selenium용)
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable
        
        # ChromeDriver는 npm으로 설치됨
      timeout-minutes: 5
    
    # 크롤링 단계
    - name: Run crawler
      if: github.event.inputs.skip_crawl != 'true'
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
        NAVER_ID: ${{ secrets.NAVER_ID }}
        NAVER_PASSWORD: ${{ secrets.NAVER_PASSWORD }}
        NAVER_COOKIES: ${{ secrets.NAVER_COOKIES }}
        HEADLESS: 'true'
        GITHUB_ACTIONS: 'true'
      timeout-minutes: 20
      run: |
        echo "🕷️ 네이버 카페 크롤링 시작"
        echo "🔐 네이버 계정: ${{ secrets.NAVER_ID }}"
        
        # Selenium 크롤러 (실제 Chrome 브라우저)
        echo "🚗 Selenium 크롤러 실행 중..."
        node crawler-selenium.js
        
        if [ $? -ne 0 ]; then
          echo "⚠️ Selenium 크롤러 실패, 대체 크롤러 시도"
          
          # Stealth 모드 크롤러
          echo "🥷 Stealth 모드 크롤러 실행 중..."
          node crawler-stealth.js || {
            echo "⚠️ Stealth 크롤러도 실패"
            
            # 로그인 없이 시도
            echo "🌐 로그인 없이 공개 게시글만 크롤링..."
            node crawler-public.js || {
              echo "❌ 모든 크롤러 실패"
              echo "⚠️ 크롤링 실패했지만 포스팅은 계속 진행합니다"
            }
          }
        fi
        
        echo "✅ 크롤링 단계 완료"
    
    # 30분 대기 (크롤링 데이터 안정화)
    - name: Wait 30 minutes after crawling
      if: github.event.inputs.skip_crawl != 'true'
      run: |
        echo "⏰ 크롤링 데이터 안정화를 위해 30분 대기 중..."
        echo "🕐 대기 시작: $(date '+%Y-%m-%d %H:%M:%S')"
        sleep 1800  # 30분 = 1800초
        echo "✅ 대기 완료: $(date '+%Y-%m-%d %H:%M:%S')"
    
    # 크롤링 건너뛴 경우 짧은 대기
    - name: Short wait for DB sync
      if: github.event.inputs.skip_crawl == 'true'
      run: sleep 5
    
    # 포스팅 단계
    - name: Run auto poster (all today's posts)
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
        NAVER_ID: ${{ secrets.NAVER_ID }}
        NAVER_PASSWORD: ${{ secrets.NAVER_PASSWORD }}
        HEADLESS: 'true'
      timeout-minutes: 30  # 10개 게시글 업로드 시간 고려
      run: |
        echo "🚀 오늘 크롤링한 모든 게시글 포스팅 시작"
        echo "📅 날짜: $(date '+%Y-%m-%d')"
        
        # 오늘자 모든 게시글 업로드
        node auto-post-all-today.js
        
        echo "✅ 포스팅 단계 완료"
    
    - name: Summary
      if: always()
      run: |
        echo "## 📊 작업 요약" >> $GITHUB_STEP_SUMMARY
        echo "- 크롤링: ${{ github.event.inputs.skip_crawl != 'true' && '실행됨' || '건너뜀' }}" >> $GITHUB_STEP_SUMMARY
        echo "- 포스팅: 실행됨" >> $GITHUB_STEP_SUMMARY
        echo "- 전체 상태: ${{ job.status }}" >> $GITHUB_STEP_SUMMARY